{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a36c6a70",
   "metadata": {},
   "source": [
    "# Part I: CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f645c015",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D  \n",
    "from keras.layers import MaxPooling2D   \n",
    "from keras.layers import Flatten        \n",
    "from keras.layers import Dense          \n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "200b6b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 623 images belonging to 3 classes.\n",
      "Found 155 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory(\n",
    "    r\"C:\\Users\\visha\\Downloads\\Shoe Classification\\Shoe Classification\\train\", \n",
    "    target_size=(136, 102),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',  \n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "validation_set = train_datagen.flow_from_directory(\n",
    "    r\"C:\\Users\\visha\\Downloads\\Shoe Classification\\Shoe Classification\\train\",  \n",
    "    target_size=(136, 102),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99d6c2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3, 3), input_shape=(136, 102, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(units=256, activation='relu'),\n",
    "    tf.keras.layers.Dense(units=len(training_set.class_indices), activation='softmax')  \n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "319a94b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 358ms/step - accuracy: 0.5489 - loss: 1.1973 - val_accuracy: 0.7812 - val_loss: 0.4680\n",
      "Epoch 2/10\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.7812 - loss: 0.3674 - val_accuracy: 0.5926 - val_loss: 0.8179\n",
      "Epoch 3/10\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 208ms/step - accuracy: 0.8234 - loss: 0.3961 - val_accuracy: 0.9062 - val_loss: 0.3262\n",
      "Epoch 4/10\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9688 - loss: 0.1972 - val_accuracy: 1.0000 - val_loss: 0.1816\n",
      "Epoch 5/10\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 226ms/step - accuracy: 0.9114 - loss: 0.2359 - val_accuracy: 0.8984 - val_loss: 0.3036\n",
      "Epoch 6/10\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9062 - loss: 0.1739 - val_accuracy: 0.9630 - val_loss: 0.2483\n",
      "Epoch 7/10\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 213ms/step - accuracy: 0.9350 - loss: 0.1725 - val_accuracy: 0.9141 - val_loss: 0.2706\n",
      "Epoch 8/10\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9688 - loss: 0.1246 - val_accuracy: 0.9259 - val_loss: 0.3142\n",
      "Epoch 9/10\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 211ms/step - accuracy: 0.9665 - loss: 0.0972 - val_accuracy: 0.9062 - val_loss: 0.4111\n",
      "Epoch 10/10\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 1.0000 - loss: 0.0321 - val_accuracy: 1.0000 - val_loss: 0.0452\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x231e0582ed0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    training_set,\n",
    "    steps_per_epoch=training_set.samples // training_set.batch_size,\n",
    "    epochs=10,\n",
    "    validation_data=validation_set,\n",
    "    validation_steps=validation_set.samples // validation_set.batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3652306d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_folder =r\"C:\\Users\\visha\\Downloads\\Shoe Classification\\Shoe Classification\\test\"\n",
    "test_images = []\n",
    "test_filenames = []\n",
    "\n",
    "for filename in os.listdir(test_folder):\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):  \n",
    "        img_path = os.path.join(test_folder, filename)\n",
    "        img = image.load_img(img_path, target_size=(136, 102))\n",
    "        img_array = image.img_to_array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        img_array /= 255.0  # Rescale\n",
    "        test_images.append(img_array)\n",
    "        test_filenames.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2ab412f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
      "Image: image (1).jpg, Predicted Class: Sandals\n",
      "Image: image (10).jpg, Predicted Class: Boots\n",
      "Image: image (11).jpg, Predicted Class: Boots\n",
      "Image: image (12).jpg, Predicted Class: Slippers\n",
      "Image: image (13).jpg, Predicted Class: Boots\n",
      "Image: image (14).jpg, Predicted Class: Sandals\n",
      "Image: image (15).jpg, Predicted Class: Boots\n",
      "Image: image (16).jpg, Predicted Class: Sandals\n",
      "Image: image (17).jpg, Predicted Class: Sandals\n",
      "Image: image (18).jpg, Predicted Class: Boots\n",
      "Image: image (19).jpg, Predicted Class: Boots\n",
      "Image: image (2).jpg, Predicted Class: Boots\n",
      "Image: image (20).jpg, Predicted Class: Boots\n",
      "Image: image (21).jpg, Predicted Class: Boots\n",
      "Image: image (22).jpg, Predicted Class: Boots\n",
      "Image: image (23).jpg, Predicted Class: Boots\n",
      "Image: image (24).jpg, Predicted Class: Sandals\n",
      "Image: image (25).jpg, Predicted Class: Sandals\n",
      "Image: image (26).jpg, Predicted Class: Sandals\n",
      "Image: image (27).jpg, Predicted Class: Sandals\n",
      "Image: image (28).jpg, Predicted Class: Sandals\n",
      "Image: image (29).jpg, Predicted Class: Sandals\n",
      "Image: image (3).jpg, Predicted Class: Boots\n",
      "Image: image (30).jpg, Predicted Class: Sandals\n",
      "Image: image (31).jpg, Predicted Class: Boots\n",
      "Image: image (32).jpg, Predicted Class: Boots\n",
      "Image: image (33).jpg, Predicted Class: Boots\n",
      "Image: image (34).jpg, Predicted Class: Boots\n",
      "Image: image (35).jpg, Predicted Class: Sandals\n",
      "Image: image (36).jpg, Predicted Class: Slippers\n",
      "Image: image (37).jpg, Predicted Class: Boots\n",
      "Image: image (38).jpg, Predicted Class: Boots\n",
      "Image: image (39).jpg, Predicted Class: Boots\n",
      "Image: image (4).jpg, Predicted Class: Boots\n",
      "Image: image (40).jpg, Predicted Class: Boots\n",
      "Image: image (41).jpg, Predicted Class: Boots\n",
      "Image: image (42).jpg, Predicted Class: Boots\n",
      "Image: image (43).jpg, Predicted Class: Boots\n",
      "Image: image (44).jpg, Predicted Class: Boots\n",
      "Image: image (45).jpg, Predicted Class: Boots\n",
      "Image: image (46).jpg, Predicted Class: Boots\n",
      "Image: image (47).jpg, Predicted Class: Boots\n",
      "Image: image (48).jpg, Predicted Class: Boots\n",
      "Image: image (49).jpg, Predicted Class: Sandals\n",
      "Image: image (5).jpg, Predicted Class: Sandals\n",
      "Image: image (50).jpg, Predicted Class: Sandals\n",
      "Image: image (51).jpg, Predicted Class: Slippers\n",
      "Image: image (52).jpg, Predicted Class: Slippers\n",
      "Image: image (53).jpg, Predicted Class: Sandals\n",
      "Image: image (54).jpg, Predicted Class: Sandals\n",
      "Image: image (55).jpg, Predicted Class: Slippers\n",
      "Image: image (56).jpg, Predicted Class: Sandals\n",
      "Image: image (57).jpg, Predicted Class: Sandals\n",
      "Image: image (58).jpg, Predicted Class: Slippers\n",
      "Image: image (59).jpg, Predicted Class: Sandals\n",
      "Image: image (6).jpg, Predicted Class: Boots\n",
      "Image: image (60).jpg, Predicted Class: Slippers\n",
      "Image: image (61).jpg, Predicted Class: Slippers\n",
      "Image: image (62).jpg, Predicted Class: Slippers\n",
      "Image: image (63).jpg, Predicted Class: Slippers\n",
      "Image: image (64).jpg, Predicted Class: Sandals\n",
      "Image: image (65).jpg, Predicted Class: Sandals\n",
      "Image: image (66).jpg, Predicted Class: Sandals\n",
      "Image: image (67).jpg, Predicted Class: Sandals\n",
      "Image: image (68).jpg, Predicted Class: Slippers\n",
      "Image: image (69).jpg, Predicted Class: Sandals\n",
      "Image: image (7).jpg, Predicted Class: Boots\n",
      "Image: image (70).jpg, Predicted Class: Slippers\n",
      "Image: image (71).jpg, Predicted Class: Slippers\n",
      "Image: image (72).jpg, Predicted Class: Sandals\n",
      "Image: image (73).jpg, Predicted Class: Slippers\n",
      "Image: image (74).jpg, Predicted Class: Slippers\n",
      "Image: image (75).jpg, Predicted Class: Sandals\n",
      "Image: image (76).jpg, Predicted Class: Slippers\n",
      "Image: image (77).jpg, Predicted Class: Slippers\n",
      "Image: image (78).jpg, Predicted Class: Slippers\n",
      "Image: image (79).jpg, Predicted Class: Sandals\n",
      "Image: image (8).jpg, Predicted Class: Sandals\n",
      "Image: image (80).jpg, Predicted Class: Slippers\n",
      "Image: image (81).jpg, Predicted Class: Slippers\n",
      "Image: image (82).jpg, Predicted Class: Slippers\n",
      "Image: image (83).jpg, Predicted Class: Sandals\n",
      "Image: image (84).jpg, Predicted Class: Sandals\n",
      "Image: image (85).jpg, Predicted Class: Slippers\n",
      "Image: image (86).jpg, Predicted Class: Sandals\n",
      "Image: image (87).jpg, Predicted Class: Sandals\n",
      "Image: image (88).jpg, Predicted Class: Sandals\n",
      "Image: image (89).jpg, Predicted Class: Slippers\n",
      "Image: image (9).jpg, Predicted Class: Slippers\n"
     ]
    }
   ],
   "source": [
    "test_images = np.vstack(test_images)\n",
    "\n",
    "predictions = model.predict(test_images)\n",
    "\n",
    "class_labels = {v: k for k, v in training_set.class_indices.items()}\n",
    "predicted_classes = [class_labels[np.argmax(prediction)] for prediction in predictions]\n",
    "\n",
    "for i, predicted_class in enumerate(predicted_classes):\n",
    "    print(f\"Image: {test_filenames[i]}, Predicted Class: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6223e4",
   "metadata": {},
   "source": [
    "# Part II: RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c02e0869",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "098eb23e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(r\"C:\\Users\\visha\\Downloads\\Tweets.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfad1076",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text airline_sentiment\n",
       "0                @VirginAmerica What @dhepburn said.           neutral\n",
       "1  @VirginAmerica plus you've added commercials t...          positive\n",
       "2  @VirginAmerica I didn't today... Must mean I n...           neutral\n",
       "3  @VirginAmerica it's really aggressive to blast...          negative\n",
       "4  @VirginAmerica and it's a really big bad thing...          negative"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[['text', 'airline_sentiment']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd655de3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@virginamerica what @dhepburn said.</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@virginamerica plus you've added commercials t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@virginamerica i didn't today... must mean i n...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@virginamerica it's really aggressive to blast...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@virginamerica and it's a really big bad thing...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text airline_sentiment\n",
       "0                @virginamerica what @dhepburn said.           neutral\n",
       "1  @virginamerica plus you've added commercials t...          positive\n",
       "2  @virginamerica i didn't today... must mean i n...           neutral\n",
       "3  @virginamerica it's really aggressive to blast...          negative\n",
       "4  @virginamerica and it's a really big bad thing...          negative"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.text= data.text.str.lower()           \n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "434aca64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c28af834",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    text_nopunt=\"\".join([c \n",
    "                         for c in text\n",
    "                         if c not in string.punctuation])\n",
    "    return text_nopunt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "488df9ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@virginamerica sfo-pdx schedule is still mia.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[15,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e123125a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'virginamerica sfopdx schedule is still mia'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.text = data.text.apply(lambda x : remove_punctuation(x))\n",
    "data.iloc[15,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4e0e2b8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'virginamerica what happened  ur vegan food options at least say on ur site so i know i wont be able  eat anything for next  hrs fail'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[26,0]\n",
    "data['text'] = data['text'].replace(r'\\d+', '', regex=True)\n",
    "data.iloc[26,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "42302eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aced68e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'virginamerica you guys messed up my seating i reserved seating with my friends and you guys gave my seat away  :enraged_face: i want free internet'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.iloc[24,0]\n",
    "def convert_emojis_to_text(text):\n",
    "    return emoji.demojize(text, language='en')\n",
    "\n",
    "data['text'] = data['text'].apply(convert_emojis_to_text)\n",
    "data.iloc[24,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4066b00a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'virginamerica you guys messed up my seating i reserved seating with my friends and you guys gave my seat away  enragedface i want free internet'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.text = data.text.apply(lambda x : remove_punctuation(x))\n",
    "data.iloc[24,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3910921c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14392, 2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape\n",
    "data =  data.drop_duplicates()\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "42947d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\visha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = text.split()\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "data['text'] = data['text'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0122fbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_mapping = {'positive': 0, 'neutral': 1, 'negative': 2}\n",
    "data['airline_sentiment'] = data['airline_sentiment'].map(sentiment_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "556d0211",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data['text'])\n",
    "vocab_size = len(tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53fcb7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "45af799f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = max([len(seq) for seq in sequences])\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1ea9524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(padded_sequences, data['airline_sentiment'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c895f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, 100),  \n",
    "    tf.keras.layers.SimpleRNN(64),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5312df44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "586ef8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 21ms/step - accuracy: 0.6552 - loss: 0.7977 - val_accuracy: 0.7121 - val_loss: 0.6718\n",
      "Epoch 2/5\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 18ms/step - accuracy: 0.8487 - loss: 0.4051 - val_accuracy: 0.7290 - val_loss: 0.6782\n",
      "Epoch 3/5\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 17ms/step - accuracy: 0.9382 - loss: 0.1881 - val_accuracy: 0.7238 - val_loss: 0.8098\n",
      "Epoch 4/5\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 16ms/step - accuracy: 0.9794 - loss: 0.0727 - val_accuracy: 0.7173 - val_loss: 0.9653\n",
      "Epoch 5/5\n",
      "\u001b[1m144/144\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.9857 - loss: 0.0529 - val_accuracy: 0.7039 - val_loss: 1.0111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x231fb87c8d0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=5, batch_size=64, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bfae5b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7091 - loss: 0.9810\n",
      "Test Accuracy: 0.7103160619735718\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Test Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b175fa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m90/90\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Confusion Matrix:\n",
      "[[ 243   94  120]\n",
      " [ 103  260  232]\n",
      " [  72  213 1542]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.53      0.56       457\n",
      "           1       0.46      0.44      0.45       595\n",
      "           2       0.81      0.84      0.83      1827\n",
      "\n",
      "    accuracy                           0.71      2879\n",
      "   macro avg       0.62      0.60      0.61      2879\n",
      "weighted avg       0.70      0.71      0.71      2879\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "y_pred = np.argmax(model.predict(X_test), axis=-1)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
